<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="map[name:Gary Sun]"><meta name=description content="An application of algebraic structure to a common computer science problem"><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://zes1092.github.io/posts/all-pairs-shortest-paths-rings-and-semi-rings/><title>All Pairs Shortest Paths, Rings and Semi Rings :: ze sheng</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=https://zes1092.github.io/main.5a556e9333891be86f81634a5853aed9841a4513870c2b6299d5a759ca00d714.css><link rel=apple-touch-icon sizes=180x180 href=https://zes1092.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://zes1092.github.io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://zes1092.github.io/favicon-16x16.png><link rel=manifest href=https://zes1092.github.io/site.webmanifest><link rel=mask-icon href=https://zes1092.github.io/safari-pinned-tab.svg color=#1b1c1d><link rel="shortcut icon" href=https://zes1092.github.io/favicon.ico><meta name=msapplication-TileColor content="#1b1c1d"><meta name=theme-color content="#1b1c1d"><meta itemprop=name content="All Pairs Shortest Paths, Rings and Semi Rings"><meta itemprop=description content="An application of algebraic structure to a common computer science problem"><meta itemprop=datePublished content="2022-03-02T00:00:00+00:00"><meta itemprop=dateModified content="2022-03-02T00:00:00+00:00"><meta itemprop=wordCount content="1811"><meta itemprop=image content="https://zes1092.github.io/"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zes1092.github.io/"><meta name=twitter:title content="All Pairs Shortest Paths, Rings and Semi Rings"><meta name=twitter:description content="An application of algebraic structure to a common computer science problem"><meta property="og:title" content="All Pairs Shortest Paths, Rings and Semi Rings"><meta property="og:description" content="An application of algebraic structure to a common computer science problem"><meta property="og:type" content="article"><meta property="og:url" content="https://zes1092.github.io/posts/all-pairs-shortest-paths-rings-and-semi-rings/"><meta property="og:image" content="https://zes1092.github.io/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-02T00:00:00+00:00"><meta property="article:modified_time" content="2022-03-02T00:00:00+00:00"><meta property="og:see_also" content="https://zes1092.github.io/posts/a-tutorial/"><meta property="og:see_also" content="https://zes1092.github.io/posts/the-stage-is-yours/"><meta property="og:see_also" content="https://zes1092.github.io/posts/for-her/"><meta property="article:section" content="cs"><meta property="article:published_time" content="2022-03-02 00:00:00 +0000 UTC"></head><body><div class=container><header class=header><span class=header__inner><a href=https://zes1092.github.io/ style=text-decoration:none><div class=logo><span class=logo__mark></span><span class=logo__text>dazed</span>
<span class=logo__cursor></span></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=https://zes1092.github.io/about/>about</a></li><li><a href=https://zes1092.github.io/posts/>posts</a></li><li><a href=https://zes1092.github.io/projects/>projects</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span><link rel=stylesheet type=text/css href=http://tikzjax.com/v1/fonts.css><script src=https://zes1092.github.io/script/tikzjax.js></script></span>
<script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-HY9N19BZYQ"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-HY9N19BZYQ")</script></header><div class=content><main class=post><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>9 minutes</p></div><article><h1 class=post-title><a href=https://zes1092.github.io/posts/all-pairs-shortest-paths-rings-and-semi-rings/>All Pairs Shortest Paths, Rings and Semi Rings</a></h1><div class=post-excerpt>An application of algebraic structure to a common computer science problem</div><hr><aside id=toc><div class=toc-title>Table of Contents</div><nav id=TableOfContents><ul><li><ul><li><a href=#introduction>Introduction</a></li><li><a href=#usual-algorithms>Usual algorithms</a></li><li><a href=#floyd-warshall-and-matrix-multiplication>Floyd-Warshall and Matrix Multiplication</a></li><li><a href=#strassens-algorithm>Strassen&rsquo;s Algorithm</a></li><li><a href=#rings-and-semi-rings>Rings and Semi-Rings</a></li><li><a href=#transitive-closure>Transitive Closure</a></li><li><a href=#conclusion>Conclusion</a></li></ul></li></ul></nav></aside><hr><div class=post-content><h2 id=introduction>Introduction</h2><p>The Single Source Shortest Paths (SSSP) problem is to find the shortest distance from one node to every other node in a graph. The All Pairs Shortest Paths (APSP) problem, is to determine for every node in the graph, the shortest distance to every other node in a graph. APSP typically has applications in routing algorithms, i.e. finding the shortest path from one location to another.</p><h2 id=usual-algorithms>Usual algorithms</h2><p>Depending on the type of graph, for both SSSP and APSP there are different algorithms to solve the relevant problems. The tables below provide the fastest well known algorithm to solve SSSP and APSP</p><h3 id=single-source-shortest-path>Single Source Shortest Path</h3><table><thead><tr><th>Graph Type</th><th>Algorithm</th><th>Runtime</th></tr></thead><tbody><tr><td>Unweighted</td><td>BFS</td><td>$O(E)$</td></tr><tr><td>Non Negative Edge Weights</td><td>Dijkstra&rsquo;s</td><td>$O(V \text{lg} V + E)$</td></tr><tr><td>General</td><td>Bellman-Ford</td><td>$O(VE)$</td></tr><tr><td>Directed Acyclic Graph (DAG)</td><td>Topological Sort + Bellman-Ford</td><td>$O(V + E)$</td></tr></tbody></table><h3 id=all-pairs-shortest-path>All Pairs Shortest Path</h3><p>Ignoring DAGs, the first two algorithms for APSP are merely SSSP algorithms run from every vertex of the graph, whilst there are different algorithms for general graphs depending on the number of edges. It is also important to note that as the number of edges increases to $O(V^2)$ in a graph, Johnson&rsquo;s algorithm approaches $O(V^2 \text{lg} V + V^3)$, making it slower than Floyd-Warshall.</p><table><thead><tr><th>Graph Type</th><th>Algorithm</th><th>Runtime</th></tr></thead><tbody><tr><td>Unweighted</td><td>$|V| \times$ BFS</td><td>$O(VE)$</td></tr><tr><td>Non Negative Edge Weights</td><td>$|V| \times$ Dijkstra&rsquo;s</td><td>$O(V^2 \text{lg} V + VE)$</td></tr><tr><td>General (Dense)</td><td>Floyd-Warshall</td><td>$O(V^3)$</td></tr><tr><td>General (Sparse)</td><td>Johnson&rsquo;s</td><td>$O(V^2 \text{lg} V + VE)$</td></tr></tbody></table><h2 id=floyd-warshall-and-matrix-multiplication>Floyd-Warshall and Matrix Multiplication</h2><p>That being said a special shout out goes to Floyd-Warshall for it&rsquo;s simplicity - elegantly composed of a few lines of code.</p><pre><code class=language-py># Implementation of the Floyd-Warshall algorithm in Python

def floyd_warshall(graph: list[list[int]]):
    n = len(graph)
    for k in range(n):
        for j in range(n):
            for i in range(n):
                graph[i][j] = min(graph[i][j], graph[i][k] + graph[k][j])
</code></pre><p>It shares a structure very similar to Matrix Multiplication (MM) if the two given matrices are square matrices (which is always the case for Floyd-Warshall as all adjacency matrices are square).</p><pre><code class=language-py># Implementation of matrix multiplication for square matrices

def matrix_mul(A: list[list[int]], B: list[list[int]]) -&gt; list[list[int]]:
    n = len(A)
    C = [[0] * n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            for k in range(n):
                C[i][j] = sum(C[i][j], A[i][k] * B[k][j])
    return C
</code></pre><p>With the exception of creating result matrix and storing the calculations there, the algorithms both run in $O(V^3)$, except for two differences in operation</p><ol><li>Instead of taking the cumulative <code>sum</code>, Floyd-Warshall takes the cumulative <code>min</code></li><li>Instead of applying <code>*</code> between two values, Floyd-Warshall applies <code>+</code></li></ol><h2 id=strassens-algorithm>Strassen&rsquo;s Algorithm</h2><p>That being said there exists a sub $O(V^3)$ time for calculating the MM of two matrices, known as Strassen&rsquo;s algorithm. It is important to note that naive MM runs in $O(V^3)$, whilst matrix addition / subtraction runs in $O(V^2)$, and so Strassen&rsquo;s algorithm aims to reuse computations and lower runtime by using less multiplications, but more addition / subtraction.</p><h3 id=explanation>Explanation</h3><p>The naive method of MM is as follows</p><p>Given the $n \times n$ matrices $A$ and $B$, to calculate their product, we can split each matrix into smaller block matrices in each quadrant (i.e. $A_{1, 1}$ is the smaller $\frac{n}{2} \times \frac{n}{2}$ matrix in the top left of A).</p><p>$$
A =
\begin{bmatrix}
A_{1, 1} A_{1, 2} \\
A_{2, 1} A_{2, 2}
\end{bmatrix}
,
B =
\begin{bmatrix}
B_{1, 1} B_{1, 2} \\
B_{2, 1} B_{2, 2}
\end{bmatrix}
,
C =
\begin{bmatrix}
C_{1, 1} C_{1, 2} \\
C_{2, 1} C_{2, 2}
\end{bmatrix}
$$</p><p>As a result the naive algorithm requires 8 multiplications (and 4 additions).</p><ol><li>$C_{1, 1} = A_{1, 1}B_{1, 1} + A_{1, 2}B_{2, 1}$</li><li>$C_{1, 2} = A_{1, 1}B_{1, 2} + A_{1, 2}B_{2, 2}$</li><li>$C_{2, 1} = A_{2, 1}B_{1, 1} + A_{2, 2}B_{2, 1}$</li><li>$C_{2, 2} = A_{2, 1}B_{1, 2} + A_{2, 2}B_{2, 2}$</li></ol><p>In comparison, Strassen&rsquo;s algorithm requires 7 multiplications (and 18 additions / subtractions) by creating temporary matrices</p><ol><li>$M_1 = (A_{1, 1} + A_{2, 2})(B_{1 ,1} + B_{2, 2})$</li><li>$M_2 = (A_{2, 1} + A_{2, 2})(B_{1, 1})$</li><li>$M_3 = (A_{1 ,1})(B_{1, 2} - B_{2, 2})$</li><li>$M_4 = (A_{2, 2})(B_{1, 2} - B_{2, 2})$</li><li>$M_5 = (A_{1, 1} + A_{1, 2})(B_{2, 2})$</li><li>$M_6 = (A_{2, 1} - A_{1, 1})(B_{1, 1} + B_{1, 2})$</li><li>$M_7 = (A_{1, 2} - A_{2, 2]})(B_{2, 1} + B_{2, 2})$</li></ol><p>which can then be added and subtracted with each other to produce the final result</p><ol><li>$C_{1, 1} = M_1 + M_4 - M_5 + M_7$</li><li>$C_{1, 2} = M_3 + M_5$</li><li>$C_{2, 1} = M_2 + M_4$</li><li>$C_{2, 2} = M_1 - M_2 + M_3 + M_6$</li></ol><h3 id=time-complexity>Time Complexity</h3><p>The final runtime of the aforementioned algorithms can be determined through the master&rsquo;s theorem
$$T(n) = aT \left( \frac{n}{b} \right) + f(n)$$
Where</p><ul><li>$a$ = Number of subproblems (no.# of smaller MMs required per subproblem)</li><li>$b$ = Reduction in size per problem (2, as the smaller matrixes have size $\frac{n}{2}$)</li><li>$f(n)$ = Cost of work per problem ($O(n^2)$, as we do matrix addition / subtraction)</li></ul><table><thead><tr><th></th><th>Naive Solution</th><th>Strassen&rsquo;s Algorithm</th></tr></thead><tbody><tr><td>Recurrence Relation</td><td>$T(n) = 8T \left( \frac{n}{2} \right) + O(n^2)$</td><td>$T(n) = 7T \left( \frac{n}{2} \right) + O(n^2)$</td></tr><tr><td>Time Complexity</td><td>$O(n^3)$</td><td>$O(n^{\text{lg}7}) \approx O(n^{2.807})$</td></tr></tbody></table><p>Not bad! We have managed to reduce the runtime to $O(n^{2.807})$, and there are algorithms that can achieve even lower asymptotic runtime (i.e. the Vassilevska Williams has a runtime of $O(n^{2.373}$), though their hidden constant times make them impractical. Now would it be possible to apply this to Floyd-Warshall?</p><p>Note: $n = V$, as $V$ is the number of vertices, which is the number of rows and columns in an adjacency matrix.</p><h2 id=rings-and-semi-rings>Rings and Semi-Rings</h2><p>The issue is that fast matrix multiplication can only be applied to any ring.</p><p>In mathematics, a ring is a set that</p><ul><li>has addition which must be commutative and associative</li><li>has multiplication that must be associative</li><li>has a zero (aka the identity element)</li><li>has negatives (i.e. adding an element and it&rsquo;s negative produces the ring&rsquo;s zero element)</li><li>has two distributive laws relating addition and multiplication</li></ul><p>However, for APSP, it is usually defined in a semi-ring. The definition of a semi-ring is the same as a ring, but <strong>without the requirement for a negative</strong>.</p><p>This becomes relevant as the Strassen&rsquo;s reduces matrix multiplications by taking advantage of matrix subtractions as the <strong>negative</strong> of the <code>sum</code> to reuse calculations.
Looking back at our comparison between Floyd-Warshall and MM, we must therefore need a negative to <code>min</code> in order to apply Strassen&rsquo;s to APSP, which unfortunately does not exist, so we cannot apply fast MMs to APSP.</p><p>But what if we could define APSP in the domain of a ring?</p><h2 id=transitive-closure>Transitive Closure</h2><p>Don&rsquo;t worry, I didn&rsquo;t drag you through all that to learn 0 applications.
The Transitive Closure for an adjacency matrix $G$ is an adjacency matrix $G&rsquo;$, where $G&rsquo;_{i, j} = 1$ if there is a path from $v_i$ to $v_j$ in $G$.</p><p>Due to the simplicity of this problem, this can be implemented through an algorithm using boolean operations as shown below.</p><h3 id=comparison-with-mm>Comparison with MM</h3><pre><code class=language-py># Implementation of a function to &quot;square&quot; a boolean matrix

def boolean_matrix_squaring(graph: list[list[bool]]):
    n = len(graph)
    for k in range(n):
        for j in range(n):
            for i in range(n):
                graph[i][j] = graph[i][j] or (graph[i][k] and graph[k][j])
</code></pre><p>We can compare this to our MM implementation and note two differences</p><ol><li>Instead of taking the cumulative <code>sum</code>, we take the cumulative <code>or</code></li><li>Instead of applying <code>*</code> between two values, we apply <code>and</code></li></ol><p>The issue earlier was that fast MM is only defined for a ring, however TC is also <a href=http://math.mit.edu/~jerison/103/handouts/rings.pdf>defined under a ring with boolean operations</a>, and as a result we can apply Strassen&rsquo;s algorithm to the TC problem.</p><h3 id=application>Application</h3><p>Now we can attempt to apply Strassen&rsquo;s algorithm to quickly find the TC of a graph.</p><h4 id=applying-with-strassens>Applying with Strassen&rsquo;s</h4><p>To find the TC, we can more or less represent <code>true</code> with <code>1</code>, <code>false</code> with <code>0</code>, and then apply Strassen&rsquo;s algorithm to multiply the adjacency matrix with itself, always taking <code>mod 2</code> of the results in $O(V^{2.807})$.</p><p>The result of adjacency matrix multiplication produces the a matrix where $A_{i, j}$ represents if there is a path of length 2 from $v_i$ to $v_j$.</p><h4 id=reduction-from-tc-to-mm>Reduction from TC to MM</h4><p>Now it is possible to reduce (apply an algorithm to convert from one problem to another) the TC problem to MM by following the following steps.</p><ol><li><p>Replace all strongly connected components with a single vertex in $O(V^2)$.</p></li><li><p>Topological sort the graph so edges move from lowered numbered vertices go to higher numbered ones in $O(V + E)$.</p></li><li><p>As a result, the adjacency matrix is an upper triangular matrix (as vertices only have a path to higher numbered vertices) $G$, which is can be composed of 4 quadrants:</p><p>$$
G =
\left[ \begin{array}{c | c}
A & C \\
\hline
0 & B \\
\end{array} \right],
$$</p><p>where $A$ is the adjacency matrix for vertices $v_1, &mldr; v_{n/2}$, and $B$ for $v_{(n/2) + 1}, &mldr;, v_{n - 1}$.
Meanwhile C represents the edges from the vertices in $A$ to $B$.</p></li></ol><p>Now, we claim that the TC of $G$:</p><p>$$
G&rsquo; =
\left[ \begin{array}{c | c}
A&rsquo; & A&rsquo; C B&rsquo; \\
\hline
0 & B&rsquo; \\
\end{array} \right].
$$</p><p>This is because the TC of $A$ is solely dependent on connections in $A$, and the same applies for $B$.</p><p>As for the upper right matrix, $C_{i, j}$ will be <code>true</code> if there is a path from $v_i$ to $v_j$, which is true if there is a path from $v_i$ to some vertex in $A&rsquo;$, which has a path to some other vertex in $B&rsquo;$ which is connected to $v_j$.</p><p>This path can be determined by taking the multiplication $A&rsquo; \times C \times B&rsquo;$ as $A&rsquo;$, hence $C&rsquo; = A&rsquo; C B&rsquo;$.</p><p>Since $G&rsquo;$ can be calculated recursively by finding the TC of 2 half sized graphs ($A&rsquo;$, $B&rsquo;$) and 2 MMs ($A&rsquo; C B&rsquo;$), the final time complexity is represented by the recurrence relation</p><p>$$
T(n) = 2T \left( \frac{n}{2} \right) + O \left( n^{2.807} \right)
$$</p><p>which can be calculated to be $O(n^{2.803})$, the same runtime as the MM algorithm used! In fact, <a href=https://www.cs.bgu.ac.il/~dinitz/Course/SS-12/transitiveClosure.pdf>TC is reducible to boolean MM</a>.</p><h3 id=repeated-dfs>Repeated DFS</h3><p>That being said, it is possible to apply DFS on every vertex of the graph to find the TC. However a single DFS of a graph has a runtime of $O(V + E)$ (which can be $O(V)$ for sparse graphs and $O(V^2)$ for dense graphs), and so the total runtime would be $O(V^2)$ for sparse graphs and $O(V^3)$ for dense graphs.</p><p>As a result the suitable algos for finding the Transitive Closure of a graph is listed below.</p><table><thead><tr><th></th><th>Dense Graphs</th><th>Sparse Graphs</th></tr></thead><tbody><tr><td>Algorithm</td><td>Strassen&rsquo;s</td><td>Repeated DFS</td></tr><tr><td>Runtime</td><td>$O(V^{2.807})$</td><td>$O(V^2)$</td></tr></tbody></table><h2 id=conclusion>Conclusion</h2><p>In conclusion, we&rsquo;ve now learn that we can apply fast MMs to other problems defined in a ring, one of which was allowing us to find the TC of a dense graph quickly by reducing the problem to MM.</p></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5a2 2 0 012-2h5l2 3h9a2 2 0 012 2z"/></svg><span class=tag><a href=https://zes1092.github.io/categories/cs/>cs</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>1811 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>2022-03-02 00:00</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://zes1092.github.io/posts/comp3821/><span class=button__icon>←</span>
<span class=button__text>COMP3821</span></a></span>
<span class="button next"><a href=https://zes1092.github.io/posts/for-her/><span class=button__text>for her</span>
<span class=button__icon>→</span></a></span></div></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2022</span>
<span><a href=https://zes1092.github.io/>ze sheng</a></span></div></div><div class=footer__inner><div class=footer__content><span></span></div></div></footer></div><script type=text/javascript src=https://zes1092.github.io/bundle.min.a2c5b062c87998f04d1b5dfb6a89a1b2d79786c21d0cb63a05e8a2082984b64b77d80955e3b97eab17273775162ba372511b711fea2f7608f216e68a67bb22d6.js integrity="sha512-osWwYsh5mPBNG137aomhsteXhsIdDLY6BeiiCCmEtkt32AlV47l+qxcnN3UWK6NyURtxH+ovdgjyFuaKZ7si1g=="></script></body></html>